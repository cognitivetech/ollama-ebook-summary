{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 234792563,
          "sourceType": "kernelVersion"
        },
        {
          "sourceId": 383161,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": true,
          "modelInstanceId": 316301,
          "modelId": 336814
        },
        {
          "sourceId": 457945,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": true,
          "modelInstanceId": 371218,
          "modelId": 392117
        }
      ],
      "dockerImageVersionId": 31011,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "## Requirements",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "%cd /kaggle/working\n!git clone https://github.com/cognitivetech/ollama-ebook-summary\n!pip install uv\n%cd ollama-ebook-summary\n!uv pip install -r requirements.txt --system\n!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python --no-cache-dir",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Imports and Core Functions",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import os, sys, csv, time, re, json\nfrom typing import List, Tuple, Optional, Dict, Any  \nfrom pathlib import Path\nfrom llama_cpp import Llama\n\n# Configuration\nPROMPTS = {\n    'bnotes': \"Write comprehensive bulleted notes summarizing the provided text, with headings and terms in bold.\",\n    'concise': \"Repeat the provided passage, with Concision.\",\n    'title': \"Write 8-11 words describing this text.\"\n}\n\nDEFAULTS = {\n    'prompt': 'bnotes',\n    'summary': 'summary_model',\n    'title': 'title_model'\n}\n\ndef get_prompt(alias: str) -> str:\n    \"\"\"Retrieve prompt by alias from the hardcoded configuration.\"\"\"\n    prompt = PROMPTS.get(alias)\n    if not prompt:\n        print(f\"Prompt alias '{alias}' not found in configuration.\")\n        sys.exit(1)\n    return prompt\n\ndef load_model(path, model_type=\"Model\"):\n    \"\"\"Load a GGUF model with error handling.\"\"\"\n    try:\n        print(f\"Loading {model_type} model from {path}...\")\n        return Llama(\n            model_path=path,\n            n_gpu_layers=-1,\n            n_ctx=4096,\n            verbose=False\n        )\n    except Exception as e:\n        print(f\"Error loading {model_type} model: {e}\")\n        sys.exit(1)\n\ndef sanitize_text(text: str) -> str:\n    \"\"\"Sanitize the input text by replacing unwanted characters.\"\"\"\n    return text.strip()\n\ndef generate_title(title_model: Llama, clean_text: str) -> Optional[str]:\n    \"\"\"Generate a unique title using the local GGUF model with few-shot prompting.\"\"\"\n    chat_prompt = [\n        {\n            \"role\": \"user\",\n            \"content\": \"```This new understanding of the multifaceted roles of the cranial nerves in health and disease opens up new therapeutic possibilities. The exercises are noninvasive and do not involve medicine or surgery.``` \\nThe content between backticks is a subsection of a book-chapter. write 8-11 words describing it.\"\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": \"Restoring Autonomic Balance Through Cranial Nerve Techniques\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"```{clean_text}``` \\nThe content between backticks is a subsection of a book-chapter. write 8-11 words describing it.\"\n        }\n    ]\n\n    try:\n        response = title_model.create_chat_completion(\n            messages=chat_prompt,\n            max_tokens=30\n        )\n        generated_title = response['choices'][0]['message']['content'].strip()\n        return generated_title.split('\\n')[0]\n    except Exception as e:\n        print(f\"Error during title generation: {e}\")\n        return None\n\ndef get_unique_title_local(original_title: str, clean_text: str, previous_original_title: str, title_model: Llama) -> Tuple[str, bool]:\n    \"\"\"Ensure the title is unique, generate a new one if necessary.\"\"\"\n    if original_title and original_title != previous_original_title:\n        return original_title, False\n\n    for _ in range(3):\n        generated_title = generate_title(title_model, clean_text)\n        if generated_title and generated_title != previous_original_title:\n            return generated_title, True\n\n    fallback_title = clean_text[:150].strip() + \"...\"\n    print(f\"Title generation failed. Using fallback title: {fallback_title}\")\n    return fallback_title, True\n\ndef bold_text_before_colon(text: str) -> str:\n    \"\"\"Bold any text before the first colon that isn't already bolded.\"\"\"\n    pattern = r'^([ \\t]*-[ \\t]*)([a-zA-Z].*?):'\n    replacement = r'\\1**\\2:**'\n    return re.sub(pattern, replacement, text)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Processing Functions",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def process_entry(clean_text: str, title: str, previous_original_title: str, \n                  summary_model: Llama, title_model: Llama, prompt_alias: str) -> Tuple[str, bool, str, float, int, str]:\n    \"\"\"Process a single text entry using local models.\"\"\"\n    unique_title, was_generated = get_unique_title_local(title, clean_text, previous_original_title, title_model)\n\n    # Choose prompt based on text length\n    if len(clean_text) < 1000:\n        prompt = get_prompt(\"concise\")\n    else:\n        prompt = get_prompt(prompt_alias)\n\n    full_prompt = f\"```{clean_text}```\\n\\n{prompt}\"\n\n    start_time = time.time()\n    \n    try:\n        response = summary_model(\n            prompt=full_prompt,\n            max_tokens=1024,\n            stop=[\"```\"],\n            echo=False\n        )\n        output = response['choices'][0]['text'].strip()\n    except Exception as e:\n        print(f\"Error during summary generation: {e}\")\n        output = \"Error: Failed to generate output.\"\n\n    end_time = time.time()\n    \n    output = bold_text_before_colon(output)\n    elapsed_time = end_time - start_time\n    size = len(output)\n    return unique_title, was_generated, output, elapsed_time, size, title\n\ndef write_csv_header(writer):\n    \"\"\"Write the CSV header.\"\"\"\n    writer.writerow([\"chapter\", \"level\", \"title\", \"text\", \"text.len\", \"summary\", \"summary.len\", \"time\"])\n\ndef write_csv_entry(writer, unique_title: str, text: str, summary: str, elapsed_time: float, is_chapter: bool, heading_level: int):\n    \"\"\"Write CSV entry.\"\"\"\n    escaped_summary = summary.replace('\\n', '\\\\n')\n    writer.writerow([\n        is_chapter, \n        heading_level, \n        unique_title, \n        text, \n        len(text), \n        escaped_summary, \n        len(summary), \n        elapsed_time\n    ])\n\ndef determine_header_level(row, default_level=3):\n    \"\"\"Determine header level from CSV row.\"\"\"\n    level = row.get('level')\n    if level:\n        try:\n            level_num = int(level)\n            return level_num + 2 if level_num == 0 else level_num\n        except ValueError:\n            print(f\"Warning: Invalid level value '{level}'. Using default level {default_level}.\")\n    return default_level\n\ndef generate_toc(toc_entries: List[Tuple[int, str]]) -> str:\n    \"\"\"Generate Table of Contents.\"\"\"\n    toc_lines = [\"## Table of Contents\"]\n    for level, text in toc_entries:\n        slug = re.sub(r'[^a-z0-9]+', '-', text.lower()).strip('-')\n        indent = \" \" * ((level - 2) * 2)\n        toc_lines.append(f\"{indent}- [{text}](#{slug})\")\n    return \"\\n\".join(toc_lines) + \"\\n\\n\"\n\ndef get_last_processed_text(csv_file: str, file_type: str) -> str:\n    \"\"\"Get the text of the last processed entry from CSV file.\"\"\"\n    try:\n        with open(csv_file, 'r', newline='', encoding='utf-8') as f:\n            reader = csv.reader(f)\n            next(reader)  # Skip header\n            text_col_idx = 3\n            last_row = None\n            for row in reader:\n                if row:\n                    last_row = row\n            return last_row[text_col_idx] if last_row else \"\"\n    except (FileNotFoundError, IndexError):\n        return \"\"",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "### Main Processing Functions",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def process_csv_input(input_file: str, summary_model: Llama, title_model: Llama, \n                    prompt_alias: str, markdown_file: str, csv_file: str,\n                    verbose: bool = False, continue_processing: bool = False):\n    \"\"\"Process CSV input files.\"\"\"\n    last_processed_text = \"\"\n    mode = \"w\"\n    markdown_lines = []\n    toc_entries = []\n\n    if continue_processing:\n        last_processed_text = get_last_processed_text(csv_file, 'csv')\n        if last_processed_text:\n            mode = \"a\"\n            print(f\"Continuing from text: {last_processed_text[:50]}...\")\n\n    with open(csv_file, mode, newline=\"\", encoding='utf-8') as csv_out:\n        writer = csv.writer(csv_out)\n        if mode == \"w\":\n            write_csv_header(writer)\n\n        skip_until_found = continue_processing and last_processed_text\n        found_last_text = not skip_until_found\n\n        with open(input_file, \"r\", encoding='utf-8') as csv_in:\n            reader = csv.DictReader(csv_in)\n            has_level_column = 'level' in reader.fieldnames\n            previous_original_title = \"\"\n\n            for row in reader:\n                text = next((row[key] for key in row if key.lower() == \"text\"), \"\").strip()\n                clean = sanitize_text(text)\n\n                if skip_until_found:\n                    if clean == last_processed_text:\n                        skip_until_found = False\n                        found_last_text = True\n                        print(f\"Resuming from: {last_processed_text[:50]}...\")\n                    continue\n\n                if not found_last_text:\n                    continue\n\n                original_title = next((row[key] for key in row if key.lower() == \"title\"), \"\").strip()\n                is_chapter = original_title and original_title != previous_original_title\n\n                if original_title == previous_original_title:\n                    unique_title, was_generated, output, elapsed_time, size, _ = process_entry(\n                        clean, \"\", previous_original_title, \n                        summary_model, title_model, prompt_alias\n                    )\n                else:\n                    unique_title, was_generated, output, elapsed_time, size, _ = process_entry(\n                        clean, original_title, previous_original_title, \n                        summary_model, title_model, prompt_alias\n                    )\n\n                base_level = determine_header_level(row) if has_level_column else 3\n                current_level = base_level + 1 if was_generated else base_level\n\n                # Handle split titles\n                if ' > ' in unique_title:\n                    parts = unique_title.split(' > ', 1)\n                    heading = f\"{'#' * current_level} {parts[0]}\\n\\n{'#' * (current_level + 1)} {parts[1]}\"\n                    toc_entries.append((current_level, parts[0]))\n                    toc_entries.append((current_level + 1, parts[1]))\n                else:\n                    heading = f\"{'#' * current_level} {unique_title}\"\n                    toc_entries.append((current_level, unique_title))\n\n                markdown_block = f\"{heading}\\n\\n{output}\\n\\n\"\n                markdown_lines.append(markdown_block)\n                \n                if verbose:\n                    print(markdown_block)\n\n                write_csv_entry(writer, unique_title, clean, output, elapsed_time, is_chapter, current_level)\n\n                if not was_generated:\n                    previous_original_title = original_title\n\n    # Generate ToC and write markdown\n    toc_content = generate_toc(toc_entries)\n    \n    with open(markdown_file, 'w', encoding='utf-8') as md_out:\n        filename_no_ext = os.path.splitext(os.path.basename(input_file))[0]\n        md_out.write(f\"# {filename_no_ext}\\n\\n\")\n        if mode == \"w\":\n            md_out.write(toc_content + \"\\n\\n\")\n        md_out.write(\"\\n\".join(markdown_lines))\n\ndef process_text_input(input_file: str, summary_model: Llama, title_model: Llama,\n                      prompt_alias: str, markdown_file: str, csv_file: str,\n                      verbose: bool = False, continue_processing: bool = False):\n    \"\"\"Process plain text input files.\"\"\"\n    mode = \"a\" if continue_processing else \"w\"\n    last_processed_text = \"\"\n\n    if continue_processing:\n        last_processed_text = get_last_processed_text(csv_file, 'txt')\n        print(f\"Continuing from text: {last_processed_text[:50]}...\")\n\n    with open(csv_file, mode, newline=\"\", encoding='utf-8') as csv_out:\n        writer = csv.writer(csv_out)\n        if mode == \"w\":\n            write_csv_header(writer)\n\n        with open(input_file, \"r\", encoding='utf-8') as txt_in:\n            previous_original_title = \"\"\n            looking_for_start = bool(continue_processing and last_processed_text)\n\n            with open(markdown_file, mode, encoding='utf-8') as md_out:\n                if mode == \"w\":\n                    filename_no_ext = os.path.splitext(os.path.basename(input_file))[0]\n                    md_out.write(f\"# {filename_no_ext}\\n\\n\")\n\n                for line in txt_in:\n                    trimmed = line.strip().strip('()')\n                    clean = sanitize_text(trimmed)\n                    extracted_title = clean[:150].strip().split('+')[0].strip()\n                    \n                    if looking_for_start:\n                        if clean == last_processed_text:\n                            looking_for_start = False\n                        else:\n                            continue\n\n                    unique_title, was_generated, output, elapsed_time, size, original_title = process_entry(\n                        clean, extracted_title, previous_original_title, \n                        summary_model, title_model, prompt_alias\n                    )\n                    unique_title = unique_title.strip('\"')\n\n                    # Remove title and '+' from text\n                    title_pattern = re.escape(unique_title)\n                    title_plus_pattern = f'(?:\"{title_pattern}\"|{title_pattern})\\\\s*\\\\+\\\\s*'\n                    clean_text = re.sub(f'^{title_plus_pattern}', '', clean, count=1).strip()\n\n                    heading = f\"#### {unique_title}\" if was_generated else f\"### {unique_title}\"\n                    markdown_text = f\"{heading}\\n\\n{output}\\n\\n\"\n                    md_out.write(markdown_text)\n                    \n                    if verbose:\n                        print(markdown_text)\n\n                    write_csv_entry(writer, unique_title, clean_text, output, elapsed_time, False, 3)\n                    previous_original_title = original_title",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Configuration and Execution",
      "metadata": {
        "_kg_hide-input": true
      }
    },
    {
      "cell_type": "code",
      "source": "# --- CONFIGURATION - MODIFY THESE VALUES ---\n# Define model paths (update these to match your Kaggle input paths)\nTITLE_MODEL_PATH = \"/kaggle/input/mistral-7b-instruct-v0.2/gguf/default/1/mistral-7b-instruct-v0.2.Q8_0.gguf\"\nSUMMARY_MODEL_PATH = \"/kaggle/input/mistral-0.2-instruct-bulleted-notes/gguf/default/1/mistral-7b-inst-0.2-bulleted-notes.Q8_0.gguf\"\n\n# Processing parameters\nINPUT_FILE = \"/kaggle/working/ollama-ebook-summary/your_input_file.csv\"  # UPDATE THIS\nPROCESSING_MODE = 'csv'  # 'csv' or 'txt'\nPROMPT_ALIAS = 'bnotes'  # 'bnotes', 'summary', or 'concise'\nVERBOSE = True\nCONTINUE_PROCESSING = False\n\n# Load models\ntitle_model = load_model(TITLE_MODEL_PATH, \"Title\")\nsummary_model = load_model(SUMMARY_MODEL_PATH, \"Summary\")\n\n# Generate output filenames\nfilename = os.path.basename(INPUT_FILE)\nfilename_no_ext, _ = os.path.splitext(filename)\nmarkdown_file = f\"{filename_no_ext}_summary.md\"\ncsv_file = f\"{filename_no_ext}_summary.csv\"\n\nprint(f\"Input file: {INPUT_FILE}\")\nprint(f\"Output files: {markdown_file}, {csv_file}\")\nprint(f\"Processing mode: {PROCESSING_MODE}\")\nprint(f\"Prompt alias: {PROMPT_ALIAS}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Run Processing",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Execute processing\nif PROCESSING_MODE == 'csv':\n    print(\"Starting CSV processing...\")\n    process_csv_input(\n        INPUT_FILE, \n        summary_model, \n        title_model, \n        PROMPT_ALIAS, \n        markdown_file, \n        csv_file, \n        VERBOSE, \n        CONTINUE_PROCESSING\n    )\nelif PROCESSING_MODE == 'txt':\n    print(\"Starting TXT processing...\")\n    process_text_input(\n        INPUT_FILE, \n        summary_model, \n        title_model, \n        PROMPT_ALIAS, \n        markdown_file, \n        csv_file, \n        VERBOSE, \n        CONTINUE_PROCESSING\n    )\nelse:\n    print(\"Error: Invalid processing mode. Use 'csv' or 'txt'.\")\n\nprint(f\"Processing completed! Output saved to {markdown_file} and {csv_file}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "",
      "metadata": {}
    }
  ]
}
